{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "S3 = True\n",
    "bucket_name = 'lrp8fruits'\n",
    "path_local = 'C:/Users/leila/openclassrooms/projet8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/ubuntu/spark-3.0.3-bin-hadoop2.7/bin\")\n",
    "#sys.path.append(\"/home/ubuntu/spark-3.0.3-bin-hadoop2.7/python\")\n",
    "#sys.path.append(\"/home/ubuntu/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] =\"/home/ubuntu/spark-3.0.3-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/ubuntu/testp8/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/testp8/bin/python'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PYSPARK_PYTHON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2 pyspark-shell'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.10.34,org.apache.hadoop:hadoop-aws:2.7.2,databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cl√©s AWS\n",
    "path_keys = os.path.join(\"/home/ubuntu/testp8/keys.csv\")\n",
    "\n",
    "with open(path_keys,'r') as f:\n",
    "        msg = f.read()\n",
    "          \n",
    "ID = str(msg).split('\\n')[0].split(';')[1]\n",
    "KEY = str(msg).split('\\n')[1].split(';')[1]\n",
    "\n",
    "# set \"temporary\" environment variables\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=KEY\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'eu-west-3'\n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, split\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import StandardScaler, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Context et SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark configuration\n",
    "#conf = (SparkConf()\n",
    "#        .set('spark.executor.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "#        .set('spark.driver.extraJavaOptions','-Dcom.amazonaws.services.s3.enableV4=true')\n",
    "#        .setAppName('p8').setMaster('local[*]'))\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .setAppName('p8').setMaster('local[*]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "# sc.setSystemProperty('com.amazonaws.services.s3.enableV4','true')\n",
    "print('modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-6-12.eu-west-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4','true')\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", ID)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", KEY)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", \"s3.eu-west-3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.impl\",\n",
    "               \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "# hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "# hadoopConf.set(\n",
    "#         \"fs.s3a.aws.credentials.provider\",\n",
    "#        \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\",\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data in a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path = path_local+'sample/*' # pour usage local\n",
    "path = \"s3a://\" + bucket_name + \"/sample/*\" #pour usgae S3\n",
    "\n",
    "images_df = spark.read.format(\"binaryfile\").load(path,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|                path|             content|    label|\n",
      "+--------------------+--------------------+---------+\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|Pineapple|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|Pineapple|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|      Fig|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|    Lemon|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|  Apricot|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|      Fig|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|    Lemon|\n",
      "|s3a://lrp8fruits/...|[FF D8 FF E0 00 1...|  Apricot|\n",
      "+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df = images_df.withColumn('label', split(col('path'),'/').getItem(4))\n",
    "images_df = images_df.select('path','content','label')\n",
    "images_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    \"\"\"\n",
    "   Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "   \"\"\"\n",
    "    model = Sequential()\n",
    "    resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "    resnet.set_weights(bc_model_weights.value)\n",
    "    model.add(resnet)\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions extraites de https://docs.databricks.com/applications/machine-learning/preprocess-data/transfer-learning-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "     Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "  # For some layers, output features will be multi-dimensional tensors.\n",
    "  # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Spark/spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\pandas\\functions.py:383: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = images_df.select(col('path'),col('label'),featurize_udf(\"content\").alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+--------------------+\n",
      "|_c0|                path|    label|            features|\n",
      "+---+--------------------+---------+--------------------+\n",
      "|  0|file:/C:/Users/33...|Pineapple|[0.0,1.8201123476...|\n",
      "|  1|file:/C:/Users/33...|Pineapple|[1.16208231449127...|\n",
      "|  2|file:/C:/Users/33...|      Fig|[11.6214609146118...|\n",
      "|  3|file:/C:/Users/33...|    Lemon|[4.68545627593994...|\n",
      "|  4|file:/C:/Users/33...|  Apricot|[5.20837974548339...|\n",
      "|  5|file:/C:/Users/33...|      Fig|[9.26463317871093...|\n",
      "|  6|file:/C:/Users/33...|    Lemon|[1.10543262958526...|\n",
      "|  7|file:/C:/Users/33...|  Apricot|[3.24856424331665...|\n",
      "+---+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat_res = spark.read.option(\"header\", 'true').csv('/home/ubuntu/feat_res2.csv')\n",
    "feat_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vect = udf(lambda l: Vectors.dense(l),VectorUDT())\n",
    "feat_res = feat_res.select(col('path'),col('label'),list_vect(feat_res['features']).alias(\"features\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(withMean=True, withStd=True,\n",
    "                    inputCol='features',\n",
    "                    outputCol = 'feats_scaled')\n",
    "\n",
    "pca = PCA(k=7, inputCol=scaler.getOutputCol(), outputCol=\"pca\")\n",
    "pipeline = Pipeline(stages=[scaler , pca])\n",
    "\n",
    "model_pca = pipeline.fit(feat_res)\n",
    "trans_feat = model_pca.transform(feat_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance cumulee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cum = model_pca.explainedVariance.cumsum()\n",
    "sns.set_context(context='poster', font_scale=0.7)\n",
    "sns.lineplot(x=[k for k in range(9)], y=np.insert(var_cum,0,0)*100, color='green')\n",
    "plt.xlabel('Nb of first components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.title('Variance cumulee')\n",
    "plt.ylim(0,100)\n",
    "plt.xlim(0,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results to S3 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- pca: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", 'true').csv('/home/ubuntu/results_pca.csv')\n",
    "final = df.select('path','label','pca')\n",
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|                path|    label|                 pca|\n",
      "+--------------------+---------+--------------------+\n",
      "|s3a://lrp8fruits/...|Pineapple|[32.1061333777547...|\n",
      "|s3a://lrp8fruits/...|Pineapple|[30.5841415756584...|\n",
      "|s3a://lrp8fruits/...|      Fig|[-25.178220340271...|\n",
      "|s3a://lrp8fruits/...|    Lemon|[-2.5568563207038...|\n",
      "|s3a://lrp8fruits/...|  Apricot|[-0.2432154889942...|\n",
      "|s3a://lrp8fruits/...|      Fig|[-29.552612833452...|\n",
      "|s3a://lrp8fruits/...|    Lemon|[-4.2925140376598...|\n",
      "|s3a://lrp8fruits/...|  Apricot|[-0.8668559323320...|\n",
      "+--------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.write.mode('overwrite').parquet(\"s3a://lrp8fruits/RESULTS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
